{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cO680Zol59t"
      },
      "source": [
        "## Open notebook in:\n",
        "| Colab                                 |  Gradient                                                                                                                                         |\n",
        "|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nicolepcx/Transformers-in-Action/blob/main/CH03/ch03_text_summarization_eval.ipynb)                                              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/Nicolepcx/Transformers-in-Action/blob/main/CH03/ch03_text_summarization_eval.ipynb)|             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vnhHxCRlHRs"
      },
      "source": [
        "# Download repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwX2FgaHT9Kd",
        "outputId": "3b8b8197-6f87-405c-8a7c-e4188d552cc5"
      },
      "outputs": [],
      "source": [
        "# Clone repo, if it's not already cloned, to be sure all runs on smoothly\n",
        "# on Colab, Kaggle or Paperspace\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('Transformers-in-Action'):\n",
        "    !git clone https://github.com/jyellow/Transformers-in-Action.git\n",
        "else:\n",
        "    print('Repository already exists. Skipping clone.')\n",
        "\n",
        "\n",
        "current_path = %pwd\n",
        "if '/Transformers-in-Action' in current_path:\n",
        "    new_path = current_path + '/utils'\n",
        "else:\n",
        "    new_path = current_path + '/Transformers-in-Action/utils'\n",
        "%cd $new_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7UpXj4bEXRi"
      },
      "source": [
        "# About this notebook\n",
        "\n",
        "In this notebook, we will explore and evaluate various text summarization models on the [BillSum dataset](https://huggingface.co/datasets/billsum). We will compare several state-of-the-art Transformer-based models, such as BART, T5, and Pegasus, as well as a TextRank-based baseline model. Our goal is to determine their performance and suitability for summarizing legislative text data.\n",
        "\n",
        "The BillSum dataset is a collection of U.S. Congressional and California state bills with their corresponding summaries. It is an interesting use case for text summarization because legislative documents tend to be lengthy, complex, and written in a formal style. Moreover, they often contain domain-specific terminology and jargon, making it challenging to generate concise and accurate summaries.\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Provide an overview of the different summarization models, including their strengths and limitations.\n",
        "2. Implement and demonstrate the use of these models to generate summaries of the legislative text.\n",
        "3. Evaluate the models on multiple samples from the BillSum dataset, considering both ROUGE and BLEU metrics for a comprehensive comparison.\n",
        "4. Discuss the importance of evaluating models on a diverse set of samples to ensure a more reliable and generalizable performance assessment.\n",
        "5. Provide insights into the BillSum dataset and why it is an interesting use case for text summarization.\n",
        "\n",
        "By the end of this notebook, you will gain a better understanding of the various text summarization models, their performance on legislative text data, and the challenges associated with evaluating models on complex and domain-specific datasets like BillSum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFkkn_xslQJr"
      },
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JxMjrTq9u_xc"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow==2.12.0 -q\n",
        "!pip install -U keras==2.12.0 -q\n",
        "!pip install -U tensorflow-gcs-config==2.12.0 -q\n",
        "!pip install -U tensorflow-probability==0.20.1 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "CJQrCNtIxt7u"
      },
      "outputs": [],
      "source": [
        "from requirements import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC7eYoG2nP4g",
        "outputId": "2bc1f30d-26a2-4c56-ba6c-55fb01e17f44"
      },
      "outputs": [],
      "source": [
        "install_base_packages()\n",
        "install_required_packages_ch03()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXhy8Pu_lVcN"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "73osbzDDpDkp"
      },
      "outputs": [],
      "source": [
        "from setup import *\n",
        "from utils import *\n",
        "from textrank import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVN6H8w-lcz8",
        "outputId": "5628934d-f15b-41c6-bca8-412a0c92a41a"
      },
      "outputs": [],
      "source": [
        "useGPU()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "DUwB04DBJVf3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import logging\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from summa import summarizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, GenerationConfig, set_seed\n",
        "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
        "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from multiprocessing import Pool\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from textwrap import TextWrapper\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vazshSUmPs-s"
      },
      "source": [
        "# Allocate enough RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEBmtiOeOOV5"
      },
      "source": [
        "Let us try to get a __GPU__ with at least __15GB RAM__ for our notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "dqsj2G_XOV5M"
      },
      "outputs": [],
      "source": [
        "# crash colab to get more RAM -> uncomment to use\n",
        "# !kill -9 -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMzVL7cLObmK"
      },
      "source": [
        "We can execute the following command `!free -h`  to see if we have enough RAM and `!nvidia-smi` to get more info about our GPU type we got assigned.\n",
        "If the allocated GPU is too small, the above cell can be used to run the command to crash the notebook hoping to get a better GPU after the crash, since the GPU is randomly allocated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRFOnJzIhLFR",
        "outputId": "51429888-79cd-4aba-b7aa-4285655876ff"
      },
      "outputs": [],
      "source": [
        "!free -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKRiHlhcOgMR",
        "outputId": "571e8203-fa9b-4296-b52d-86900d06e7a4"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuwH1xvZviMy",
        "outputId": "ddb93a8a-053e-4a08-e0b8-209fdb943e38"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    gpu_device = torch.device('cuda')\n",
        "    gpu_info = torch.cuda.get_device_properties(gpu_device)\n",
        "    gpu_memory = gpu_info.total_memory / 1e9  # Convert bytes to gigabytes\n",
        "    print(f\"GPU: {gpu_info.name}, Total Memory: {gpu_memory:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsqe0wSIm1on"
      },
      "source": [
        "# Use case BillSum dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7tR4s4AnBLq"
      },
      "source": [
        "## Load the BillSum dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "95636e27788a42bb956277754119de3c",
            "4193d4a9d30f483a9ac7e7de2321659c",
            "0458f06434d0423292f3e900a7644085",
            "f12c83e5682747f99fe9d201d9a91df1",
            "2258216fed674b5c994f6a7614a806c5",
            "a5ab9bf1cd9d4982a7114f1ebb64c604",
            "e1a62d96648f431baff1d4539f68bd0d",
            "46689cccec994c6d95245b29788ce71b",
            "33ac1be49fb645e9812e709c2ce7496e",
            "5edcc604831b42dcb9c20b81a7aae44f",
            "ec30904800c447149397063ae62c4057"
          ]
        },
        "id": "05A_omTEridT",
        "outputId": "060934c5-cddd-4ec4-8595-d0415c5c467a"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"billsum\")\n",
        "#test_dataset.set_format(type='pandas', columns=['article', 'abstract'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptnAT3cJJAlb",
        "outputId": "e7f24e80-b2b2-4627-fe4f-9e2e4ec52f98"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCfJOZ_t72Ne",
        "outputId": "6f90a045-927f-4ea5-cdb3-3649d2825e0d"
      },
      "outputs": [],
      "source": [
        "# Get the test split of the dataset\n",
        "dataset_test = load_dataset(\"billsum\", split=\"ca_test\")\n",
        "dataset_test.set_format(type=\"pandas\", columns=['text', 'summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "7VMWmWT_Wzg8"
      },
      "outputs": [],
      "source": [
        "# Convert to pandas DataFrame\n",
        "df = dataset_test.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbJHhoVjnLj6"
      },
      "source": [
        "## Excerpt of article and ground truth of summary\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asiwMOOC7_O5",
        "outputId": "452b96fa-eaf8-45f7-e824-bba8fa2af60d"
      },
      "outputs": [],
      "source": [
        "# Access first row using iloc\n",
        "sample = df.iloc[0]\n",
        "\n",
        "excerpt = 1000\n",
        "\n",
        "print(f\"\\033[1mExcerpt of {excerpt} characters, total length of article: \\\n",
        "{len(sample['text'])}:\\033[0m\\n\")\n",
        "\n",
        "print(sample[\"text\"][:excerpt])\n",
        "print(f\"\\033[1m\\n\\nSummary (length: {len(sample['summary'])}):\\033[0m\\n\")\n",
        "print(sample[\"summary\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8mQZi889ban",
        "outputId": "1062d18d-ff3d-43f7-d97d-392395bfbd26"
      },
      "outputs": [],
      "source": [
        "articles_np= np.array(dataset_test['text'])\n",
        "articles_np.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fq9mjuZ8d3E",
        "outputId": "74b3d255-16c7-46e8-9211-792fb3014663"
      },
      "outputs": [],
      "source": [
        "articles_np_1 = articles_np[:1]\n",
        "articles_np_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz3rwLgS9qa5",
        "outputId": "97017077-b484-4649-f136-60e69ce63a32"
      },
      "outputs": [],
      "source": [
        "print(articles_np_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbuQUiFHnhmM"
      },
      "source": [
        "## Applying TextRank algorithm to BillSum dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-59vmQm2HD5S"
      },
      "source": [
        "This code snippet is used to generate summaries for a collection of texts (in this case, legislative documents) using the TextRank algorithm. The TextRank summarization is implemented with the summa library, which is available on [PyPI](https://pypi.org/project/summa/). The implementation is based on the paper Mihalcea, R., Tarau, P.: [“TextRank: Bringing order into texts”](https://aclanthology.org/W04-3252/)\n",
        "\n",
        "A `summarize_text` function is defined, which takes in two arguments - text and words. text is the input text to be summarized, and words is an optional parameter to specify the target number of words in the generated summary (default is 250 words). The `summarizer.summarize()` function from the summa library is used to generate the summary.\n",
        "\n",
        "The code then employs the multiprocessing Pool class to parallelize the summarization process. This speeds up the computation by taking advantage of multiple CPU cores available on the system. The `Pool()` context manager creates a pool of worker processes, and the `pool.map()` function is used to apply the `summarize_text` function to each element of the `articles_np_1` array (which contains the input articles).\n",
        "\n",
        "The `pool.map()` function distributes the articles across the available worker processes, and each process applies the summarize_text function independently. Once all the summaries are generated, they are collected and stored in the `summarized_articles` list. This parallelization can significantly improve the performance of the TextRank summarization, especially when working with large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "QVuIQytjsEU7"
      },
      "outputs": [],
      "source": [
        "# Define the function to summarize the text\n",
        "def summarize_text(text, words=250):\n",
        "    summary = summarizer.summarize(text, words=words)\n",
        "    return summary\n",
        "\n",
        "# Parallelize the TextRank summarization\n",
        "with Pool() as pool:\n",
        "    summarized_articles = pool.map(summarize_text, articles_np_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkUYrYNwnzG7"
      },
      "source": [
        "### Get summarized article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du-G1Yz29wYL",
        "outputId": "8fc99321-94d8-4d9b-8637-fdf796fa2e23"
      },
      "outputs": [],
      "source": [
        "print(summarized_articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "1cYq9QIPDey9"
      },
      "outputs": [],
      "source": [
        "references = np.array(dataset_test['summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cguTgWVxBZCy",
        "outputId": "e20e2767-5b57-4599-d7ee-92510ae1a70d"
      },
      "outputs": [],
      "source": [
        "references_np = np.array(references)\n",
        "print(references_np[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "0HFQvehJ-Ej_"
      },
      "outputs": [],
      "source": [
        "reference = references[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXMcOiNvWz_7",
        "outputId": "be8f60da-b005-4efa-fdcc-e26ca06b34ef"
      },
      "outputs": [],
      "source": [
        "print(summarized_articles[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "jANpwhitKuD4"
      },
      "outputs": [],
      "source": [
        "# Create a dict to hold our summaries\n",
        "summaries = {}\n",
        "summaries['TextRank (Baseline)'] = summarized_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtSqhykcg-XG",
        "outputId": "1c3679cd-b78f-4e48-ed0b-9b9bb3c7a75f"
      },
      "outputs": [],
      "source": [
        "print(summaries['TextRank (Baseline)'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56n0hJw3oHrs"
      },
      "source": [
        "# Appyling various Transformer models to the BillSum dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb4L2bik2mhO"
      },
      "source": [
        "Here we introduce a function called `generate_summary` that allows you to generate a summary of a given input text using various pre-trained transformer models. The function supports the following models:\n",
        "\n",
        "- BART [Modelcard](https://huggingface.co/facebook/bart-large-cnn), [Paper](https://arxiv.org/abs/1910.13461)\n",
        "- T5 [Modelcard](https://huggingface.co/sysresearch101/t5-large-finetuned-xsum-cnn), [Paper](https://arxiv.org/abs/1910.10683)\n",
        "- ProphetNet [Modelcard](), [Paper](https://arxiv.org/abs/2001.04063)\n",
        "- Pegasus [Modelcard](https://huggingface.co/google/pegasus-cnn_dailymail), [Paper](https://arxiv.org/abs/1912.08777)\n",
        "\n",
        "__Function: generate_summary__ <br>\n",
        "The `generate_summary` function takes two arguments: `model_name` and `sample_text`. The model_name argument is a string that specifies the transformer model to use for summarization, while the `sample_text` argument is the input text to be summarized.\n",
        "\n",
        "__Usage__\n",
        "\n",
        "To use the generate_summary function, simply call it with the desired model name and the input text:\n",
        "\n",
        "`summary = generate_summary(\"BART\", \"This is an example text to summarize using BART.\")\n",
        "print(summary)\n",
        "`\n",
        "\n",
        "__Models and Tokenizers__\n",
        "\n",
        "The function sets up the tokenizer and model using the `AutoTokenizer` and `AutoModelForSeq2SeqLM` classes from the Hugging Face Transformers library. The model and tokenizer are specified in the `model_dict` dictionary:\n",
        "\n",
        "`model_dict = {\n",
        "    \"BART\": \"facebook/bart-large-cnn\",\n",
        "    \"T5\": \"sysresearch101/t5-large-finetuned-xsum-cnn\",\n",
        "    \"ProphetNet\": \"microsoft/prophetnet-large-uncased-cnndm\"\n",
        "}\n",
        "`\n",
        "\n",
        "__Handling Text Chunks__\n",
        "\n",
        "Some transformer models have a __maximum input length constraint__. To handle this, the input text is broken down into smaller chunks before being passed to the pipeline. The chunks are processed one by one, and the generated summaries are concatenated to form the final summary.\n",
        "\n",
        "\n",
        "__Error Handling__\n",
        "\n",
        "If the specified model_name is not supported, the function raises a ValueError.\n",
        "\n",
        "__Example__\n",
        "\n",
        "Here's an example of how to use the generate_summary function:\n",
        "\n",
        "`summary = generate_summary(\"BART\", \"This is an example text to summarize using BART.\")\n",
        "print(summary)\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "iCDTR7w5tTnF"
      },
      "outputs": [],
      "source": [
        "def generate_summary(model_name, sample_text):\n",
        "    \"\"\"\n",
        "    Generate a summary of the input text using a specified model.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the model to use for summarization. Supported values are \"BART\", \"T5\",\n",
        "            \"ProphetNet\", \"Pegasus\", and \"GPT-2\".\n",
        "        sample_text (str): The input text to summarize.\n",
        "\n",
        "    Returns:\n",
        "        str: The summary generated by the specified model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the specified model is not supported.\n",
        "\n",
        "    Notes:\n",
        "        - If `model_name` is \"Pegasus\", the `google/pegasus-cnn_dailymail` model will be used for summarization.\n",
        "          This model doesn't require a tokenizer or a maximum length.\n",
        "        - If `model_name` is \"GPT-2\", the `gpt2-xl` model will be used for text generation. The summary will be\n",
        "          generated by extracting the first paragraph from the generated text that follows the \"TL;DR:\" token.\n",
        "\n",
        "        - For other models, the `generate_summary` function uses the `pipeline` method from the transformers library to\n",
        "          generate the summary. Since some of these models have a maximum input length constraint, the input text needs to\n",
        "          be broken down into smaller chunks before being passed to the `pipeline`. The size of the chunks is determined\n",
        "          by the `max_length` parameter of the tokenizer used for the specified model. The chunks are processed one by one,\n",
        "          and the generated summaries are concatenated to form the final summary.\n",
        "\n",
        "    Example:\n",
        "        >>> generate_summary(\"BART\", \"This is an example text to summarize using BART.\")\n",
        "        'BART is used to summarize the input text.'\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up tokenizer and model\n",
        "    model_dict = {\n",
        "        \"BART\": \"facebook/bart-large-cnn\",\n",
        "        \"T5\": \"sysresearch101/t5-large-finetuned-xsum-cnn\",\n",
        "        \"ProphetNet\": \"microsoft/prophetnet-large-uncased-cnndm\"\n",
        "    }\n",
        "\n",
        "    if model_name == \"Pegasus\":\n",
        "        # Generate summary using the specified model\n",
        "        summarization_pipeline = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
        "        chunks = [sample_text[i:i+2048] for i in range(0, len(sample_text), 2048)]\n",
        "        summaries = []\n",
        "        for chunk in chunks:\n",
        "            summarization_output = summarization_pipeline(chunk, max_length=512)\n",
        "            summaries.append(summarization_output[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))\n",
        "        summary = \" \".join(summaries)\n",
        "    elif model_name in model_dict.keys():\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_dict[model_name], max_length=1024, truncation=True)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_dict[model_name])\n",
        "        gen_config = GenerationConfig(max_length=512)\n",
        "        summarization_pipeline = pipeline(\"summarization\", tokenizer=tokenizer, model=model, config=gen_config)\n",
        "        # Generate summary using the specified model\n",
        "        chunks = [sample_text[i:i+1024] for i in range(0, len(sample_text), 1024)]\n",
        "        summaries = []\n",
        "        for chunk in chunks:\n",
        "            summarization_output = summarization_pipeline(chunk)\n",
        "            summaries.append(summarization_output[0][\"summary_text\"])\n",
        "        summary = \" \".join(summaries)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} is not supported.\")\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "QbVz3rGQvEzi"
      },
      "outputs": [],
      "source": [
        "# Hide transformers output\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8pgp9hRoVMi"
      },
      "source": [
        "## Generate summary from  <font color='red'>\"Pegasus\"</font> model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "7gbyu2T1BkoO"
      },
      "outputs": [],
      "source": [
        "sample_text = sample[0]\n",
        "summaries[\"Pegasus\"] = generate_summary(\"Pegasus\", sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_7HQQKepVrw"
      },
      "source": [
        "## Generate summary from  <font color='red'>\"BART\"</font> model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "_GbRC8FX0Fgd"
      },
      "outputs": [],
      "source": [
        "summaries[\"BART\"] = generate_summary(\"BART\", sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYV1aTalpeMq"
      },
      "source": [
        "## Generate summary from  <font color='red'>\"T5\"</font> model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxuDO-sl0GJF",
        "outputId": "8f9c51d7-4baf-46bc-b75c-56cb223e6909"
      },
      "outputs": [],
      "source": [
        "summaries[\"T5\"] = generate_summary(\"T5\", sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXdzjMvqpihJ"
      },
      "source": [
        "## Generate summary from  <font color='red'>\"ProphetNet\"</font> model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "hve3X4st0fQa"
      },
      "outputs": [],
      "source": [
        "summaries[\"ProphetNet\"] = generate_summary(\"ProphetNet\", sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZRMfTHGWJV7"
      },
      "source": [
        "## Generate summary from  <font color='red'>\"BigBirdPegasus\"</font> model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePiFNa8mWjaN"
      },
      "source": [
        "BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this [paper](https://arxiv.org/abs/2007.14062), [Modelcard](https://huggingface.co/google/bigbird-pegasus-large-arxiv).\n",
        "<br><br>\n",
        "\n",
        "__Model description__\n",
        "\n",
        "BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\n",
        "\n",
        "The models checkpoint is obtained after fine-tuning BigBirdPegasusForConditionalGeneration for summarization on arxiv dataset from scientific papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Zx_YFmpIKVON"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\", block_size=16, num_random_blocks=2)\n",
        "\n",
        "text = sample_text\n",
        "inputs = tokenizer.encode_plus(text, return_tensors='pt', max_length=4096, truncation=True)\n",
        "prediction = model.generate(**inputs, max_length=512)  # Set max_length here\n",
        "prediction = tokenizer.batch_decode(prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "jXm6KAvZMzOm"
      },
      "outputs": [],
      "source": [
        "summaries['bigbird-pegasus'] = prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Xu663-Y-UrIv"
      },
      "outputs": [],
      "source": [
        "summaries['bigbird-pegasus'] = summaries['bigbird-pegasus'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCKXQBmLEJ9Z",
        "outputId": "ce5c88c5-3e91-4884-f3e4-fac52bb8b91a"
      },
      "outputs": [],
      "source": [
        "print_summaries(summaries, reference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI4QFvDd9KE5"
      },
      "source": [
        "### Evaluating Summarization Models using ROUGE and BLEU Metrics\n",
        "\n",
        "In this section, we demonstrate how to evaluate the summarization models using both the ROUGE and BLEU metrics. We use the Hugging Face's implementation of these metrics.\n",
        "\n",
        "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n",
        "[Link](https://huggingface.co/spaces/evaluate-metric/rouge)\n",
        "\n",
        "BLEU, or Bilingual Evaluation Understudy, is an evaluation metric used primarily for machine translation. It measures the similarity between a candidate translation and a set of reference translations, considering both the n-gram precision and a brevity penalty factor.\n",
        "[Link](https://huggingface.co/spaces/evaluate-metric/google_bleu)\n",
        "\n",
        "[Info about Huggingface evaluate](https://huggingface.co/docs/evaluate/v0.4.0/en/package_reference/loading_methods#evaluate.load)\n",
        "<br> <br>\n",
        "\n",
        "__Note:__ It can be beneficial to use both ROUGE and BLEU metrics for evaluation, as they focus on different aspects of the generated summaries. ROUGE is recall-oriented and mainly focuses on the coverage of important content, while BLEU is precision-oriented and evaluates the fluency and correctness of the generated text.\n",
        "\n",
        "It is important to have an understanding of what constitutes a \"good\" score when evaluating summarization models. Here are some guidelines for interpreting the ROUGE and BLEU scores:\n",
        "\n",
        "__ROUGE Scores__\n",
        "\n",
        "ROUGE scores usually range from 0 to 1, with 1 being a perfect match between the generated summary and the reference summary. In practice, scores closer to 1 are rare, especially for abstractive summarization tasks.\n",
        "\n",
        "For a simple baseline model like TextRank or LSA, a \"good\" ROUGE score can be around 0.3 to 0.4. For more advanced Transformer-based models like BART, T5, or Pegasus, a \"good\" ROUGE score can range from 0.4 to 0.6, depending on the task and dataset. State-of-the-art models can achieve even higher scores, but it's important to keep in mind that ROUGE scores are just one aspect of evaluating the quality of generated summaries.\n",
        "\n",
        "__BLEU Scores__\n",
        "\n",
        "BLEU scores also range from 0 to 1, with 1 indicating a perfect match between the generated text and the reference text. However, BLEU scores tend to be lower compared to ROUGE scores, as they are more sensitive to the differences in word order and phrasing.\n",
        "\n",
        "For a simple baseline model, a \"good\" BLEU score could be around 0.1 to 0.2. For Transformer-based models, a \"good\" BLEU score might range from 0.2 to 0.4, again depending on the task and dataset. State-of-the-art models can achieve higher scores, but just like with ROUGE, it's essential to consider other factors when evaluating the quality of generated summaries.\n",
        "\n",
        "Keep in mind that these are rough guidelines, and the definition of a \"good\" score can vary depending on the specific domain, dataset, and evaluation criteria. It is always a good idea to compare your model's scores with scores from existing state-of-the-art models on the same dataset to get a better understanding of its performance.\n",
        "\n",
        "__Defining the SummarizationMetrics Class__\n",
        "\n",
        "The SummarizationMetrics class is defined with methods for computing the ROUGE and BLEU metrics, as well as a method to compute both metrics together:\n",
        "\n",
        "`class SummarizationMetrics:\n",
        "    ...\n",
        "`\n",
        "\n",
        "This class is located in the `utils.py` file, feel free to look it up to get a better understanding how the mectrics are computed.\n",
        "\n",
        "__Computing Metrics__\n",
        "\n",
        "To compute the metrics, we first instantiate the SummarizationMetrics class:\n",
        "`evaluator = SummarizationMetrics()\n",
        "`\n",
        "\n",
        "Then, we call the compute_sum_metric method on the evaluator object, passing the generated summaries and the reference summary:\n",
        "`metrics_df = evaluator.compute_sum_metric(summaries, reference)\n",
        "`\n",
        "\n",
        "The compute_sum_metric method computes both the ROUGE and BLEU metrics by calling the `compute_rouge_metrics` and `compute_google_bleu_metrics` methods, respectively. The resulting metrics are combined into a single DataFrame and returned.\n",
        "\n",
        "__Viewing the Metrics__\n",
        "\n",
        "We can display the resulting metrics DataFrame to see the evaluation scores for each model:\n",
        "`metrics_df\n",
        "`\n",
        "\n",
        "This DataFrame shows the ROUGE and BLEU scores for each summarization model, allowing for an easy comparison of their performance.\n",
        "\n",
        "\n",
        "__Note:__ It's crucial to understand that evaluating a summarization model based on just one text example from a testing dataset is not enough to determine its overall performance. The reason behind this is that the complexity of the text samples can vary significantly within a dataset. Some text samples might be easier for a model to summarize, while others could be more challenging, containing complex sentence structures, domain-specific jargon, or long and convoluted narratives.\n",
        "\n",
        "To get a more comprehensive sense of a model's performance, it is essential to evaluate it on a larger number of samples from the testing dataset. By doing so, we can ensure that we're assessing the model's ability to handle a diverse set of inputs, including those that differ in complexity, topic, and structure. This approach provides a more reliable and generalizable evaluation of the model's performance, helping us to better understand its strengths and weaknesses.\n",
        "\n",
        "In the next section of this notebook, we will expand our evaluation to include multiple text samples from the testing dataset. This will give us a better understanding of how well the model performs across a variety of inputs and allow us to make more informed decisions about its applicability to real-world use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "1ajjw6rcKyQ7",
        "outputId": "4bd26169-7ead-4026-8766-7ad797720bb0"
      },
      "outputs": [],
      "source": [
        "evaluator = SummarizationMetrics()\n",
        "\n",
        "metrics_df = evaluator.compute_sum_metric(summaries, reference)\n",
        "\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE272kth5H7E"
      },
      "source": [
        "## Evaluate more than one example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvK9YAoG5QTU"
      },
      "source": [
        "The `generate_avg_summary` is almost identical to `generate_summary`. However the main differences are:\n",
        "\n",
        "1. The choice of models: While both functions support \"BART\", \"T5\", \"ProphetNet\", and \"Pegasus\", the generate_avg_summary function also supports the \"BigBirdPegasus\" model, which is designed for handling longer input sequences.\n",
        "\n",
        "2. Handling long input text: The generate_avg_summary function takes advantage of the BigBirdPegasus model's ability to handle longer input sequences (up to 4096 tokens). This can be particularly useful for summarizing long documents or articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "nm_Z5Tyb-Z9Z"
      },
      "outputs": [],
      "source": [
        "def generate_avg_summary(model_name, sample_text):\n",
        "    \"\"\"\n",
        "    This function generates an average summary for the given text using the\n",
        "    specified transformer model. Different transformer models such as Pegasus,\n",
        "    T5, and others are supported.\n",
        "    Each model has its own tokenizer and configuration settings. For example,\n",
        "    Pegasus and T5 use a maximum length of 1024, while BigBirdPegasus has a\n",
        "    maximum length of 4096. The input text is chunked according to the model's\n",
        "    maximum length and processed accordingly. The final summary is obtained by\n",
        "    concatenating the summaries generated for each chunk.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the model to use for summarization. Supported values are \"BART\", \"T5\",\n",
        "            \"ProphetNet\", \"Pegasus\", and \"BigBirdPegasus\".\n",
        "        sample_text (str): The input text to summarize.\n",
        "\n",
        "    Returns:\n",
        "        str: The summary generated by the specified model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the specified model is not supported.\n",
        "\n",
        "    Example:\n",
        "        >>> generate_summary(\"BART\", \"This is an example text to summarize using BART.\")\n",
        "        'BART is used to summarize the input text.'\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up tokenizer and model\n",
        "    model_dict = {\n",
        "        \"BART\": (\"facebook/bart-large-cnn\", None),\n",
        "        \"T5\": (\"t5-large\", None),\n",
        "        \"ProphetNet\": (\"microsoft/prophetnet-large-uncased-cnndm\", None),\n",
        "        \"Pegasus\": (\"google/pegasus-cnn_dailymail\", 1024),\n",
        "        \"BigBirdPegasus\": (\"google/bigbird-pegasus-large-arxiv\", 4096),\n",
        "    }\n",
        "\n",
        "    if model_name in model_dict:\n",
        "        if model_name == \"Pegasus\":\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dict[model_name][0], max_length=1024, truncation=True)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_dict[model_name][0])\n",
        "            summarization_pipeline = pipeline(\"summarization\", tokenizer=tokenizer, model=model, max_length=model_dict[model_name][1])\n",
        "            chunks = [sample_text[i:i+model_dict[model_name][1]] for i in range(0, len(sample_text), model_dict[model_name][1])]\n",
        "            summaries = []\n",
        "            for chunk in tqdm(chunks):\n",
        "                summarization_output = summarization_pipeline(chunk)\n",
        "                summaries.append(summarization_output[0][\"summary_text\"])\n",
        "            summary = \" \".join(summaries)\n",
        "        elif model_name == \"BigBirdPegasus\":\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dict[model_name][0], max_length=4096, truncation=True)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_dict[model_name][0])\n",
        "            inputs = tokenizer(sample_text, return_tensors='pt', max_length=4096, truncation=True)\n",
        "            outputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "            summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        else:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dict[model_name][0], model_max_length=1024, truncation=True)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_dict[model_name][0])\n",
        "            if model_name == \"T5\":\n",
        "                gen_config = GenerationConfig(max_length=1024)\n",
        "                summarization_pipeline = pipeline(\"summarization\", tokenizer=tokenizer, model=model, config=gen_config)\n",
        "            else:\n",
        "                summarization_pipeline = pipeline(\"summarization\", tokenizer=tokenizer, model=model, max_length=512)\n",
        "            chunks = [sample_text[i:i+1024] for i in range(0, len(sample_text), 1024)]\n",
        "            summaries = []\n",
        "            for chunk in tqdm(chunks):\n",
        "                summarization_output = summarization_pipeline(chunk)\n",
        "                summaries.append(summarization_output[0][\"summary_text\"])\n",
        "            summary = \" \".join(summaries)\n",
        "\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} is not supported.\")\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYzvb6N55qyj"
      },
      "source": [
        "## Summarizing a Small Dataset using Transformer variants and `generate_avg_summary`\n",
        "\n",
        "In this section, we demonstrate how to use the `generate_avg_summary` function to generate summaries for a small dataset using the various models. The dataset in this example is assumed to be in the `dataset_test`\n",
        " variable, which is an instance of Hugging Face's Dataset class.\n",
        "\n",
        "__Selecting a Small Range of Samples__\n",
        "\n",
        "First, we select a small range of samples from the dataset. In this example, we select the first 10 samples:\n",
        "`# Select a small range of samples\n",
        "dataset_small = dataset_test.select(range(10))\n",
        "`\n",
        "\n",
        "__Converting the Dataset to a Pandas DataFrame__\n",
        "Next, we convert the small dataset to a Pandas DataFrame, which provides an easier way to apply the generate_avg_summary function on each article:\n",
        "`# Convert dataset to pandas dataframe\n",
        "df = dataset_small.to_pandas()\n",
        "`\n",
        "\n",
        "__Generating Summaries using BART, T5, Pegasus and our baseline, TextRank__\n",
        "\n",
        "Now that we have a DataFrame containing the articles, we can apply the __generate_avg_summary__ function to each article to generate BART-based summaries. For each article, a progress bar will appear, indicating the progress of the summarization process. Since we have selected 10 articles, 10 progress bars will be displayed.\n",
        "`# Generate summaries for each article with BART\n",
        "df[\"BART_avg\"] = df[\"text\"].apply(lambda x: generate_avg_summary(\"BART\", x))\n",
        "`\n",
        "\n",
        "After the summarization process is complete, the resulting DataFrame df will contain a new column named \"Modl_avg\" with the Model-generated summaries for each article. You can adjust the `.select(range(10)` with different ranges to evaluate less or more articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAPcVETNdHWF",
        "outputId": "c129b7e4-1e9d-4ed8-8b1d-580fe081c51b"
      },
      "outputs": [],
      "source": [
        "# Select a small range of samples\n",
        "# See: https://huggingface.co/docs/datasets/v1.1.1/processing.html\n",
        "dataset_small = dataset_test.select(range(10))\n",
        "\n",
        "# Convert dataset to pandas dataframe\n",
        "df = dataset_small.to_pandas()\n",
        "\n",
        "# Note: every bar appearing is == to one data iteration, so if you have\n",
        "# dataset_test.select(range(10)) 10 bars will apprear\n",
        "# Generate summaries for each article with BART\n",
        "df[\"BART_avg\"] = df[\"text\"].apply(lambda x: generate_avg_summary(\"BART\", x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOsGdx6EdTD2",
        "outputId": "1592fffd-edf6-4f98-bd33-d1553d30fbbd"
      },
      "outputs": [],
      "source": [
        "# Generate summaries for each article with T5\n",
        "df[\"T5_avg\"] = df[\"text\"].apply(lambda x: generate_avg_summary(\"T5\", x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R69zfWSshaQB",
        "outputId": "c4c69ac7-0721-449f-956b-aabac549793e"
      },
      "outputs": [],
      "source": [
        "# Generate summaries for each article with Pegasus\n",
        "df[\"Pegasus_avg\"] = df[\"text\"].apply(lambda x: generate_avg_summary(\"Pegasus\", x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2ayPpy0SMuo",
        "outputId": "6ed547e8-e38e-4371-e1b6-9877a37d5b91"
      },
      "outputs": [],
      "source": [
        "# get 10 examples\n",
        "articles_np_10 = articles_np[:10]\n",
        "articles_np_10.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "iUQVRzGNSMup"
      },
      "outputs": [],
      "source": [
        "# Define a partial new function with the `words` argument fixed to 120\n",
        "summarize_text_120 = partial(summarize_text, words=250)\n",
        "\n",
        "# Parallelize the TextRank summarization\n",
        "with Pool() as pool:\n",
        "    summarized_articles = pool.map(summarize_text_120, articles_np_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "h119uhb8Sd58"
      },
      "outputs": [],
      "source": [
        "df['TextRank (Baseline)_avg'] = summarized_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzsI3yFW7YEG"
      },
      "source": [
        "## Evaluating Summarization Models using ROUGE Metrics\n",
        "\n",
        "In this section, we demonstrate how to evaluate the summarization models using the ROUGE metrics. ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n",
        "\n",
        "The evaluation is performed using the Hugging Face's implementation of ROUGE, which is a wrapper around the Google Research reimplementation of ROUGE.\n",
        "\n",
        "The `compute_rouge` function, which calculates the ROUGE scores, is located in the `utils.py` file. This file is assumed to have been imported from a GitHub repository, and you can refer to it for more insights on the implementation of the compute_rouge function.\n",
        "\n",
        "__Defining Model Names__\n",
        "\n",
        "First, we define a list of model names that we want to evaluate. In this example, we include the TextRank (baseline), BART, T5, and Pegasus models:\n",
        "`model_names = [\"TextRank (Baseline)\", \"BART\", \"T5\", \"Pegasus\"]\n",
        "`\n",
        "\n",
        "__Computing ROUGE Metrics__\n",
        "\n",
        "Next, we use the compute_rouge function to calculate the ROUGE scores for each of the models:\n",
        "`results = compute_rouge(df, model_names)\n",
        "`\n",
        "\n",
        "The compute_rouge function takes the DataFrame df and the list of model names as inputs, and returns a dictionary containing the ROUGE scores for each model.\n",
        "\n",
        "__Creating a DataFrame for Average Metrics__\n",
        "\n",
        "To better visualize and analyze the results, we can convert the results dictionary to a Pandas DataFrame:\n",
        "`avg_metrics = pd.DataFrame.from_dict(results)\n",
        "`\n",
        "\n",
        "__Displaying the Results__\n",
        "\n",
        "Finally, we display the DataFrame containing the average ROUGE metrics for each model:\n",
        "`avg_metrics.T\n",
        "`\n",
        "\n",
        "By examining the DataFrame, you can compare the performance of the different summarization models and decide which one best suits your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "e9hMndoB0NNO"
      },
      "outputs": [],
      "source": [
        "model_names = [\"TextRank (Baseline)\", \"BART\", \"T5\", \"Pegasus\"]\n",
        "results = compute_rouge(df, model_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "xvEOhIzcrQfp"
      },
      "outputs": [],
      "source": [
        "results_bleu = compute_bleu(df, model_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "_dMrhnjz0_3g"
      },
      "outputs": [],
      "source": [
        "avg_metrics = pd.DataFrame.from_dict(results)\n",
        "avg_metrics = avg_metrics.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "O8Ud9zok5Di9",
        "outputId": "4b2e2d69-1ea9-4a86-dc8a-61ba9e04c2bd"
      },
      "outputs": [],
      "source": [
        "avg_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "2N0XbXkg5cfH",
        "outputId": "13e1f494-e20e-4ac2-8eda-8d193c823ae6"
      },
      "outputs": [],
      "source": [
        "df_bleu = pd.DataFrame.from_dict(results_bleu)\n",
        "df_bleu = df_bleu.T\n",
        "df_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "vVIuYW4s5qfn",
        "outputId": "070bcfce-bc2d-4007-fd5f-e72e8bc695e5"
      },
      "outputs": [],
      "source": [
        "pd.concat([avg_metrics, df_bleu], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "d4BUFDSY5-tH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "history_visible": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0458f06434d0423292f3e900a7644085": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46689cccec994c6d95245b29788ce71b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33ac1be49fb645e9812e709c2ce7496e",
            "value": 3
          }
        },
        "2258216fed674b5c994f6a7614a806c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ac1be49fb645e9812e709c2ce7496e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4193d4a9d30f483a9ac7e7de2321659c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5ab9bf1cd9d4982a7114f1ebb64c604",
            "placeholder": "​",
            "style": "IPY_MODEL_e1a62d96648f431baff1d4539f68bd0d",
            "value": "100%"
          }
        },
        "46689cccec994c6d95245b29788ce71b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5edcc604831b42dcb9c20b81a7aae44f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95636e27788a42bb956277754119de3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4193d4a9d30f483a9ac7e7de2321659c",
              "IPY_MODEL_0458f06434d0423292f3e900a7644085",
              "IPY_MODEL_f12c83e5682747f99fe9d201d9a91df1"
            ],
            "layout": "IPY_MODEL_2258216fed674b5c994f6a7614a806c5"
          }
        },
        "a5ab9bf1cd9d4982a7114f1ebb64c604": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a62d96648f431baff1d4539f68bd0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec30904800c447149397063ae62c4057": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f12c83e5682747f99fe9d201d9a91df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5edcc604831b42dcb9c20b81a7aae44f",
            "placeholder": "​",
            "style": "IPY_MODEL_ec30904800c447149397063ae62c4057",
            "value": " 3/3 [00:00&lt;00:00, 176.12it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
